{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2E Platform Demo",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JVavDq-Lxh"
      },
      "source": [
        "## Welcome to the Abacus.AI Developer Workshop\n",
        "This workbook provides you with a hands on environment to take an existing python model and deploy it into Abacus.AI\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- [Sign up](https://abacus.ai/app/signup?signupToken=python_models) for an Abacus.AI Account\n",
        "- Once your account is created, navigate to the [API Keys Dashboard](https://abacus.ai/app/profile/apikey) and generate an API key to authenticate your ApiClient\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ejrLq_-Zl6"
      },
      "source": [
        "## How-to guide for custom Python data transforms and models\n",
        "This notebook provides you with a hands on environment to build and deploy custom python models in the Abacus.AI environment. Custom here refers to both the data transformations required to build a model and the training process used to construct a model from data. Having the custom logic hosted in Abacus.AI then allows for the process to be run automatically to refresh the model as new data arrives and to host the model for the generation of online or batch predictions. In addition, it allows for additional features like monitoring input drift, model performance and other ML Ops requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhrIhXeqJJE"
      },
      "source": [
        "1. Install the Abacus.AI library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsm7EQrmvey"
      },
      "source": [
        "!pip install abacusai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izrh6I2aqlTa"
      },
      "source": [
        "2. Add your Abacus.AI [API Key](https://abacus.ai/app/profile/apikey) generated using the API dashboard as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8TJ_-Qamz5x"
      },
      "source": [
        "#@title Abacus.AI API Key\n",
        "\n",
        "api_key = ''  #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqpb-UfFqmzs"
      },
      "source": [
        "3. Import the Abacus.AI library and instantiate a client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8M1MoB0m1DV"
      },
      "source": [
        "from abacusai import ApiClient, ApiException\n",
        "client = ApiClient(api_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXzo8ZRqodi"
      },
      "source": [
        "## 1. Create a Project\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSjEJ7AqrQW"
      },
      "source": [
        "In this notebook, we're going to see how to use python to customize models in Abacus. We will cover custom data transforms, model training and prediction handling. Projects that will be hosting a custom model needed to be created with the `PYTHON_MODEL` use case. Note that custom python data transforms can be used in any kind of project and like any other feature group can be shared across projects. However, custom training algorithms and prediction functions are enabled by this use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8idfft0im5ci"
      },
      "source": [
        "project = client.create_project(name='Demo Python Model', use_case='PYTHON_MODEL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8T5vcLrIlg"
      },
      "source": [
        "## 2. Creating Datasets\n",
        "\n",
        "Abacus.AI can read datasets directly from File blob storage\n",
        "\n",
        "We are going to use a single dataset for this project.\n",
        "- [Concrete Strength](https://s3.amazonaws.com/abacusai.exampledatasets/predicting/concrete_measurements.csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R3p7tAkrwPv"
      },
      "source": [
        "### Add the datasets to Abacus.AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mDVrJYKrzB9"
      },
      "source": [
        "Using the Create Dataset API, we can tell Abacus.AI the public S3 URI of where to find the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi6qwR46m71i"
      },
      "source": [
        "# if the dataset already exists, skip creation\n",
        "try: \n",
        "  concrete_dataset = client.describe_dataset(client.describe_feature_group_by_table_name('concrete_strength').dataset_id)\n",
        "except ApiException: # dataset not found\n",
        "  concrete_dataset = client.create_dataset_from_file_connector(\n",
        "      name='Concrete Strength',\n",
        "      table_name='concrete_strength',\n",
        "      location='s3://abacusai.exampledatasets/predicting/concrete_measurements.csv')\n",
        "  concrete_dataset.wait_for_inspection()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRKjHmJDYsZS"
      },
      "source": [
        "### Load the dataset so we can build and test the transform.\n",
        "\n",
        "Most of the time it is easiest to develop custom transformations on your local machine. It makes iteration, inspection and debugging easier and often you can do it directly in a notebook environment. To enable simple local development you can use the Abacus.AI client to load your dataset as a pandas dataframe. This tends to work well if your dataset is under `100MB` but for datasets that get much larger you will likely want to construct a sampled feature group for development.\n",
        "\n",
        "Here we are working with a fairly small dataset so can easily load it into memory. The first block fetches the feature group corresponding to the dataset (datasets are used to move data into Abacus.AI, feature groups are used to consume data for various operations). It initiates a materialization of the feature group to generate a snapshot, waits for it to be ready and then loads it as a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egrfrygYfU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "ff07ca43-224f-49bc-9eb7-0e6205b2a493"
      },
      "source": [
        "concrete_feature_group = concrete_dataset.describe_feature_group()\n",
        "if not concrete_feature_group.list_versions():\n",
        "  concrete_feature_group.create_version()\n",
        "concrete_feature_group.wait_for_materialization()\n",
        "\n",
        "concrete_df = concrete_feature_group.load_as_pandas()\n",
        "concrete_df[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cement</th>\n",
              "      <th>slag</th>\n",
              "      <th>flyash</th>\n",
              "      <th>water</th>\n",
              "      <th>superplasticizer</th>\n",
              "      <th>coarseaggregate</th>\n",
              "      <th>fineaggregate</th>\n",
              "      <th>age</th>\n",
              "      <th>csMPa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365.0</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360.0</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>266.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>670.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>47.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>380.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365.0</td>\n",
              "      <td>43.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>380.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>36.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>266.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>670.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>45.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>475.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>39.29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cement   slag  flyash  water  ...  coarseaggregate  fineaggregate    age  csMPa\n",
              "0   540.0    0.0     0.0  162.0  ...           1040.0          676.0   28.0  79.99\n",
              "1   540.0    0.0     0.0  162.0  ...           1055.0          676.0   28.0  61.89\n",
              "2   332.5  142.5     0.0  228.0  ...            932.0          594.0  270.0  40.27\n",
              "3   332.5  142.5     0.0  228.0  ...            932.0          594.0  365.0  41.05\n",
              "4   198.6  132.4     0.0  192.0  ...            978.4          825.5  360.0  44.30\n",
              "5   266.0  114.0     0.0  228.0  ...            932.0          670.0   90.0  47.03\n",
              "6   380.0   95.0     0.0  228.0  ...            932.0          594.0  365.0  43.70\n",
              "7   380.0   95.0     0.0  228.0  ...            932.0          594.0   28.0  36.45\n",
              "8   266.0  114.0     0.0  228.0  ...            932.0          670.0   28.0  45.85\n",
              "9   475.0    0.0     0.0  228.0  ...            932.0          594.0   28.0  39.29\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YycOfEOgZSiP"
      },
      "source": [
        "#### Custom Data Transform\n",
        "We are going to transform the dataset so that flyash is no longer a feature but instead all the other values are transformed according to whether they have `flyash > 0` or not.\n",
        "\n",
        "The example is not entirely realistic and it is certainly feasible to achieve the same result using SQL. However, the point is to illustrate that you are free to transform the dataset using the full functionality of python and its data frameworks. Here we are using pandas but you can use a wide range of standard python libraries to manipulate the data. Additionally, you can bundle resources with your code, for example small maps or tables, that can be accessed by your function to implement the transform.\n",
        "\n",
        "Note that we test the function locally by running it against the dataframe loaded from the feature group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSpGvxlsZHlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "914b9742-fde4-4173-c7b0-f5b2eeff59e4"
      },
      "source": [
        "def transform_concrete(concrete_dataset):\n",
        "  import pandas as pd\n",
        "  feature_df = concrete_dataset.drop(['flyash'], axis=1)\n",
        "  no_flyash = feature_df[concrete_dataset.flyash == 0.0]\n",
        "  flyash = feature_df[concrete_dataset.flyash > 0.0]\n",
        "  return pd.concat([no_flyash - no_flyash.assign(age=0).mean(), flyash - flyash.assign(age=0).mean()])\n",
        "\n",
        "concrete_by_flyash_df = transform_concrete(concrete_df)\n",
        "concrete_by_flyash_df[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cement</th>\n",
              "      <th>slag</th>\n",
              "      <th>water</th>\n",
              "      <th>superplasticizer</th>\n",
              "      <th>coarseaggregate</th>\n",
              "      <th>fineaggregate</th>\n",
              "      <th>age</th>\n",
              "      <th>csMPa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>225.962191</td>\n",
              "      <td>-100.110247</td>\n",
              "      <td>-24.616784</td>\n",
              "      <td>-1.555654</td>\n",
              "      <td>66.64258</td>\n",
              "      <td>-88.853004</td>\n",
              "      <td>28.0</td>\n",
              "      <td>43.218216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>225.962191</td>\n",
              "      <td>-100.110247</td>\n",
              "      <td>-24.616784</td>\n",
              "      <td>-1.555654</td>\n",
              "      <td>81.64258</td>\n",
              "      <td>-88.853004</td>\n",
              "      <td>28.0</td>\n",
              "      <td>25.118216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.462191</td>\n",
              "      <td>42.389753</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-170.853004</td>\n",
              "      <td>270.0</td>\n",
              "      <td>3.498216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.462191</td>\n",
              "      <td>42.389753</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-170.853004</td>\n",
              "      <td>365.0</td>\n",
              "      <td>4.278216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-115.437809</td>\n",
              "      <td>32.289753</td>\n",
              "      <td>5.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>5.04258</td>\n",
              "      <td>60.646996</td>\n",
              "      <td>360.0</td>\n",
              "      <td>7.528216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-48.037809</td>\n",
              "      <td>13.889753</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-94.853004</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.258216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>65.962191</td>\n",
              "      <td>-5.110247</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-170.853004</td>\n",
              "      <td>365.0</td>\n",
              "      <td>6.928216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>65.962191</td>\n",
              "      <td>-5.110247</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-170.853004</td>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.321784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-48.037809</td>\n",
              "      <td>13.889753</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-94.853004</td>\n",
              "      <td>28.0</td>\n",
              "      <td>9.078216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>160.962191</td>\n",
              "      <td>-100.110247</td>\n",
              "      <td>41.383216</td>\n",
              "      <td>-4.055654</td>\n",
              "      <td>-41.35742</td>\n",
              "      <td>-170.853004</td>\n",
              "      <td>28.0</td>\n",
              "      <td>2.518216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       cement        slag      water  ...  fineaggregate    age      csMPa\n",
              "0  225.962191 -100.110247 -24.616784  ...     -88.853004   28.0  43.218216\n",
              "1  225.962191 -100.110247 -24.616784  ...     -88.853004   28.0  25.118216\n",
              "2   18.462191   42.389753  41.383216  ...    -170.853004  270.0   3.498216\n",
              "3   18.462191   42.389753  41.383216  ...    -170.853004  365.0   4.278216\n",
              "4 -115.437809   32.289753   5.383216  ...      60.646996  360.0   7.528216\n",
              "5  -48.037809   13.889753  41.383216  ...     -94.853004   90.0  10.258216\n",
              "6   65.962191   -5.110247  41.383216  ...    -170.853004  365.0   6.928216\n",
              "7   65.962191   -5.110247  41.383216  ...    -170.853004   28.0  -0.321784\n",
              "8  -48.037809   13.889753  41.383216  ...     -94.853004   28.0   9.078216\n",
              "9  160.962191 -100.110247  41.383216  ...    -170.853004   28.0   2.518216\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGSdSuifcyzC"
      },
      "source": [
        "### Registering Python Functions\n",
        "\n",
        "Now that we have a working transform the next step is to register it with Abacus.AI to allow it to run the function when required by workflows. For simple self-contained functions we can just pass the function to the client and it will build a suitable resource to ship the python code to Abacus.AI. For more complicated functions and in cases where additional resources are required you can instead build an archive and add it to the registration function.\n",
        "\n",
        "Registering the function involves supplying the source artifact, the name of the function implementing the transform and a list of required input feature groups. These feature groups will be passed as dataframe arguments to the functions. Optionally, you can also supply configuration options as keywork arguments that can alter the behavior of the function. For example, the same function may be used to construct two different feature groups differing only in the keyword arguments.\n",
        "\n",
        "Note, that Abacus.AI will ensure that the function is operating on the latest versions of the input feature groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7siYsLdIZ3bL"
      },
      "source": [
        "concrete_flyash = client.create_feature_group_from_function(\n",
        "    table_name='concrete_by_flyash',\n",
        "    function_source_code=transform_concrete,\n",
        "    function_name='transform_concrete',\n",
        "    input_feature_groups=['concrete_strength'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGJy2yoPa8kV"
      },
      "source": [
        "concrete_flyash.create_version()\n",
        "concrete_flyash.wait_for_materialization()\n",
        "concrete_by_flyash_df = concrete_flyash.load_as_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pv5fjeObtrF"
      },
      "source": [
        "### Custom Model\n",
        "Now we will define a custom model trained on this flyash partitioned data. A custom training function is similar in many ways to a custom transform. The main difference being instead of returning a new dataframe with the transformed data it returns an object containing the trained model. It is required that object returned should be pickleable by the standard python `pickle` library. However, the model is free to serialize additional data to local disk in the current working directory. The contents of the working directory will be made available at prediction time. There is support for supplying an initialization function along with prediction function that will receive the unpickled model object and transform it based on data loaded from disk to use at prediction. This will be covered in more detail later.\n",
        "\n",
        "To illustrate that the training can be customized arbitrarily we will train a composite model that depending on the age of the concrete uses a linear model on quantized features or a GBDT trained on raw inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZruY_-2bssl"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMjC2lvc-RIs"
      },
      "source": [
        "Just like with data transforms we can test our function locally to ensure it works on the data frame as expected and that it is building a reasonable model. Notice that the model object we return is tuple comprising\n",
        "- columns used as inputs to the sub models\n",
        "- the quantile transform\n",
        "- linear model\n",
        "- catboost model\n",
        "\n",
        "Since this tuple can be pickled we do not need to bother writing anything to local disk. Also we will be able to use the default identity initialization function which will just return this tuple unmodified at prediction time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38W5NRR0cBd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4f770d-af81-4cdd-a772-0fffca0ac763"
      },
      "source": [
        "def train(training_dataset):\n",
        "  # set the seed for reproduceable results\n",
        "  import numpy as np\n",
        "  np.random.seed(5)\n",
        "\n",
        "  X = training_dataset.drop(['csMPa'], axis=1)\n",
        "  y = training_dataset.csMPa\n",
        "  recent = training_dataset.age < 10\n",
        "  from sklearn.preprocessing import QuantileTransformer\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  qt = QuantileTransformer(n_quantiles=20)\n",
        "  recent_model = LinearRegression()\n",
        "  _ = recent_model.fit(qt.fit_transform(X[recent]), y[recent])\n",
        "  print(f'Linear model R^2 = {recent_model.score(qt.transform(X[recent]), y[recent])}')\n",
        "\n",
        "  from catboost import Pool, CatBoostRegressor\n",
        "  train_pool = Pool(X[~recent], y[~recent])\n",
        "  older_model = CatBoostRegressor(iterations=20, depth=2, loss_function='RMSE')\n",
        "  _ = older_model.fit(train_pool)\n",
        "  metrics = older_model.eval_metrics(train_pool, ['RMSE'])\n",
        "  old_r2 = 1 - metrics['RMSE'][-1]**2 / y[~recent].var()\n",
        "  print(f'Catboost model R^2 = {old_r2}')\n",
        "\n",
        "  return (X.columns, qt, recent_model, older_model)\n",
        "\n",
        "local_model = train(concrete_by_flyash_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear model R^2 = -59474.80409065779\n",
            "Learning rate set to 0.5\n",
            "0:\tlearn: 12.7627412\ttotal: 46.8ms\tremaining: 187ms\n",
            "1:\tlearn: 11.5585084\ttotal: 47.8ms\tremaining: 71.7ms\n",
            "2:\tlearn: 10.3223491\ttotal: 48.4ms\tremaining: 32.3ms\n",
            "3:\tlearn: 9.3247540\ttotal: 49.1ms\tremaining: 12.3ms\n",
            "4:\tlearn: 8.5430952\ttotal: 49.9ms\tremaining: 0us\n",
            "Catboost model R^2 = 0.6814947748102853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:439: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woQK4LYrdEI8"
      },
      "source": [
        "### Prediction Function\n",
        "\n",
        "To actually use this model for predictions we need to tell the service how to evaluate a new input against the returned model object. This function could be as simple as calling `predict()` on a scikit model compliant model. However, it will usually be the case that there will be some translation of the request data into model inputs prior to the final invocation. This is to match any feature engineering / transformation done inside the training function. Keep in mind any feature transformation done in feature group transformations will be handled automatically by the service for batch predictions.\n",
        "\n",
        "In the example we are building there is even more complexity because the model is a composite model built on two partitions of the data so the prediction function needs to dispatch the input to the right model based on one of the input features.\n",
        "\n",
        "We can follow the same pattern of testing locally to ensure that the prediction function works as expected. If the model requires an initialization function that loads data from disk it would also be good to test that locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4tWAjFdRj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd65a5b1-2924-4d9c-f59f-0760675aeb06"
      },
      "source": [
        "def predict(model, query):\n",
        "  # abacusai.get_client().get_feature_group().lookup(...)\n",
        "  columns, qt, recent_model, older_model = model\n",
        "  import pandas as pd\n",
        "  X = pd.DataFrame({c: [query[c]] for c in columns})\n",
        "  if X.age[0] < 10:\n",
        "    y = recent_model.predict(qt.transform(X))[0]\n",
        "  else:\n",
        "    y = older_model.predict(X.values.reshape(-1))\n",
        "  return {'csMPa': y}\n",
        "\n",
        "for _, r in concrete_by_flyash_df[concrete_by_flyash_df.age < 10][:5].iterrows():\n",
        "  print(predict(local_model, r.to_dict()), r['csMPa'])\n",
        "\n",
        "for _, r in concrete_by_flyash_df[concrete_by_flyash_df.age > 10][:5].iterrows():\n",
        "  print(predict(local_model, r.to_dict()), r['csMPa'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'csMPa': -31.75412474980192} -28.711784452296826\n",
            "{'csMPa': -5.324797742032455} 1.8282155477031736\n",
            "{'csMPa': -4.377726654712578} -1.6917844522968295\n",
            "{'csMPa': -23.147157848108026} -21.721784452296827\n",
            "{'csMPa': -16.712019233341156} -10.511784452296826\n",
            "{'csMPa': 19.340295162698652} 43.21821554770317\n",
            "{'csMPa': 19.340295162698652} 25.118215547703173\n",
            "{'csMPa': 10.376285866501192} 3.4982155477031753\n",
            "{'csMPa': 10.376285866501192} 4.278215547703169\n",
            "{'csMPa': -2.273645085750303} 7.528215547703169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoDQfssCeTF8"
      },
      "source": [
        "### Register the Model\n",
        "We now put together the feature group, the training function and the prediction function as a new Abacus model. Like with custom feature groups the model has to specify the feature groups required for training which will be passed as arguments to the train function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8_QX3WheaSO"
      },
      "source": [
        "model = client.create_model_from_functions(project_id=project, \n",
        "                                   train_function=train, \n",
        "                                   predict_function=predict, \n",
        "                                   training_input_tables=['concrete_by_flyash'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv49KPlg0sWR"
      },
      "source": [
        "Wait for the model to finish training and then deploy the model to use for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_y39zZOfaky"
      },
      "source": [
        "model.wait_for_training()\n",
        "print(model.latest_model_version.get_training_logs(stdout=True, stderr=True)[0])\n",
        "deployment_token = client.create_deployment_token(project).deployment_token\n",
        "deployment = client.create_deployment(model_id=model)\n",
        "deployment.wait_for_deployment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbqwEs9l1DRp"
      },
      "source": [
        "Now we can run predictions on Abacus and compare against predictions from the local model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfr0OUi1hzpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad8d512-9705-4bfb-944e-adb9f5e245d7"
      },
      "source": [
        "# locally trained\n",
        "for _, r in concrete_by_flyash_df[concrete_by_flyash_df.age < 10][:5].iterrows():\n",
        "  print(predict(local_model, r.to_dict()), r['csMPa'])\n",
        "\n",
        "print(' Is equal to ')\n",
        "\n",
        "# remotely trained\n",
        "for _, r in concrete_by_flyash_df[concrete_by_flyash_df.age < 10][:5].iterrows():\n",
        "  print(client.predict(deployment_token, deployment.deployment_id, r.to_dict()), r['csMPa'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'csMPa': -31.75412474980192} -28.711784452296826\n",
            "{'csMPa': -5.324797742032455} 1.8282155477031736\n",
            "{'csMPa': -4.377726654712578} -1.6917844522968295\n",
            "{'csMPa': -23.147157848108026} -21.721784452296827\n",
            "{'csMPa': -16.712019233341156} -10.511784452296826\n",
            " Is equal to \n",
            "{'csMPa': -31.75412474980191} -28.711784452296826\n",
            "{'csMPa': -5.324797742032466} 1.8282155477031736\n",
            "{'csMPa': -4.377726654712589} -1.6917844522968295\n",
            "{'csMPa': -23.14715784810803} -21.721784452296827\n",
            "{'csMPa': -16.712019233341152} -10.511784452296826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3xHkP81OJi"
      },
      "source": [
        "### Setup Batch Predictions\n",
        "\n",
        "We can setup a new dataset to feed a batch prediction job. Abacus will run the prediction dataset through the feature transformation function and then apply the custom model to generate predictions for the uploaded data. Keep in mind the input to the model will be what is generated by transform. The inputs to the model are included in the batch prediction download along with model outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNGk-GNcKEZ0"
      },
      "source": [
        "try: \n",
        "  prediction_dataset = client.describe_dataset(client.describe_feature_group_by_table_name('concrete_strength_prediction_input').dataset_id)\n",
        "except ApiException: # dataset not found\n",
        "  prediction_dataset = client.create_dataset_from_file_connector(\n",
        "      name='Concrete Strength Prediction Input',\n",
        "      table_name='concrete_strength_prediction_input',\n",
        "      location='s3://abacusai.exampledatasets/predicting/concrete_measurements.csv')\n",
        "  prediction_dataset.wait_for_inspection()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2aEtvCQ-Ug"
      },
      "source": [
        "Now we create a new batch prediction template and set its input to point to the prediction dataset we created. This will configure it to use the latest version of the dataset as the input for each run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwoQZgHcQ3Af"
      },
      "source": [
        "batch_prediction = client.create_batch_prediction(deployment.deployment_id)\n",
        "batch_prediction.set_dataset_remap({\n",
        "    concrete_dataset.dataset_id: prediction_dataset.dataset_id\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYmtCk00RTmM"
      },
      "source": [
        "Finally, we initiate a run of the batch prediction template. This is the actual call that initiates the prediction job. It will handle any required transformation on the input dataaset to match what was done at training time and then apply the custom model deployed to each row of input produced by the custom data transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tDUzgRuQ9A_"
      },
      "source": [
        "batch_prediction_run = batch_prediction.start()\n",
        "batch_prediction_run.wait_for_predictions()\n",
        "with open('/tmp/batch_predictions_results.json', 'wb') as bpr_file:\n",
        "  batch_prediction_run.download_result_to_file(bpr_file)\n",
        "!head /tmp/batch_predictions_results.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1slz4UV12HX"
      },
      "source": [
        "### Attach Refresh Schedules\n",
        "\n",
        "As a final step we can attach refresh schedules to various objects to ensure that they are updated regularly without any manual intervention. This allows the custom model to run with the same level of automation as models generated internally by the service."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xGS8RJ2HdR"
      },
      "source": [
        "concrete_dataset.create_refresh_policy('0 4 * * 1')\n",
        "prediction_dataset.create_refresh_policy('0 4 * * 1')\n",
        "model.create_refresh_policy('0 6 * * 1')\n",
        "batch_prediction.create_refresh_policy('0 8 * * 1')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
